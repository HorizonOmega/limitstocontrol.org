<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Limits to Control Workshop 2025</title>
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Roboto+Slab:wght@400;700&display=swap"
        rel="stylesheet">
    <style>
        body {
            font-family: 'Lato', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
            /* Lighter gray background */
            color: #333;
            line-height: 1.7;
        }

        .hero {
            background-color: #4A0E0E;
            /* Base dark red */
            background-image:
                /* Fine grid */
                repeating-linear-gradient(0deg, rgba(255, 255, 255, 0.04), rgba(255, 255, 255, 0.04) 1px, transparent 1px, transparent 15px),
                repeating-linear-gradient(90deg, rgba(255, 255, 255, 0.04), rgba(255, 255, 255, 0.04) 1px, transparent 1px, transparent 15px),
                /* Larger, fainter diagonal schematic lines */
                repeating-linear-gradient(45deg, transparent, transparent 30px, rgba(255, 255, 255, 0.03) 30px, rgba(255, 255, 255, 0.03) 32px, transparent 32px, transparent 70px),
                repeating-linear-gradient(-45deg, transparent, transparent 30px, rgba(255, 255, 255, 0.03) 30px, rgba(255, 255, 255, 0.03) 32px, transparent 32px, transparent 70px);
            background-size: 15px 15px, 15px 15px, 70px 70px, 70px 70px;
            /* Size for each layer */
            color: white;
            text-align: center;
            padding: 60px 20px 40px 20px;
            position: relative;
            /* For potential future pseudo-elements */
        }

        .hero h1 {
            font-size: 2.8em;
            /* Reduced font size */
            margin-bottom: 10px;
            font-weight: 700;
            font-family: 'Roboto Slab', serif;
        }

        .hero-underline {
            width: 100px;
            height: 4px;
            background-color: #7E57C2;
            /* More vibrant purple accent */
            margin: 0 auto 30px auto;
            /* Adjusted margin */
        }

        .container {
            width: 85%;
            max-width: 850px;
            margin: 30px auto;
            padding: 35px;
            background-color: white;
            box-shadow: 0 2px 15px rgba(0, 0, 0, 0.08);
            border-radius: 8px;
        }

        h2 {
            font-family: 'Roboto Slab', serif;
            color: #7E57C2;
            font-size: 1.9em;
            margin-top: 40px;
            margin-bottom: 25px;
            border-bottom: 2px solid #efefef;
            padding-bottom: 12px;
        }

        h3 {
            font-family: 'Roboto Slab', serif;
            color: #555;
            font-size: 1.5em;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        p,
        ul,
        li {
            margin-bottom: 16px;
            font-size: 1.05em;
        }

        ul {
            list-style-type: disc;
            padding-left: 25px;
        }

        li {
            margin-bottom: 10px;
        }

        a {
            color: #7E57C2;
            text-decoration: none;
            font-weight: bold;
        }

        a:hover,
        a:focus {
            text-decoration: underline;
            color: #5E35B1;
        }

        footer {
            text-align: center;
            padding: 30px 20px;
            margin-top: 50px;
            background-color: #333;
            color: #bbb;
            font-size: 0.95em;
        }

        hr {
            border: 0;
            height: 1px;
            background: #e0e0e0;
            margin: 50px 0;
        }
    </style>
</head>

<body>
    <header class="hero">
        <h1>Limits to Control<br>Workshop 2025</h1>
        <div class="hero-underline"></div>
    </header>

    <main class="container">
        <section id="intro">
            <h2>Limits to Control</h2>
            <p>Can we keep enough control over AI? If systems are developed to be more and more autonomous, this is no
                longer a given. It's a hypothesis, calling for serious investigation.</p>
            <p>Even alignment relies on control. Researchers build mechanisms to control AI's impacts in line with human
                values.</p>
            <p>So how must a control mechanism operate? What limits its capacity to track and correct all the AI
                signals/effects? Does it provide enough stability, or give way eventually to runaway impacts?</p>
            <p>We explore these questions in a new field: Limits to Control.</p>
        </section>

        <section id="about-workshop">
            <h2>About the workshop</h2>
            <p>This in-person workshop is meant to facilitate deep collaboration. We're bringing together researchers to
                map the territory of AI control limitations – to understand the dynamics, patterns, and impossibilities
                of control. We are thrilled to welcome Roman Yampolskiy, Anders Sandberg, Forrest Landry, and other
                researchers working on control limitations.</p>
            <p>This workshop aims to:</p>
            <ul>
                <li>Build common knowledge among researchers on the limits to AI control.</li>
                <li>Facilitate high-fidelity discussions through whiteboard sessions and collaborative problem-solving.
                </li>
                <li>Identify and clarify viable directions in control research by establishing boundaries on
                    controllability.</li>
                <li>Formalize and elevate this critical research topic.</li>
            </ul>
        </section>

        <section id="dates-location">
            <h2>Dates & location</h2>
            <ul>
                <li>Date: Wednesday, June 11 - Friday, June 13, 2025</li>
                <li>Location: University of Louisville, Kentucky, USA.</li>
            </ul>
            <p>Detailed logistical information will be provided to confirmed participants.</p>
        </section>

        <section id="agenda">
            <h2>Agenda</h2>
            <p>We aim to strike a balance between structured sharing and messy exploration – we believe this is where
                the best ideas tend to emerge. Over the three days, we will do:</p>
            <ul>
                <li>Talks: Researchers present their current work, keeping it under an hour.</li>
                <li>Discussions: We break out into groups to discuss specific questions and bounce ideas around.</li>
                <li>Regrouping: We come back into one room to synthesize what came out of the discussions.</li>
                <li>Next steps: On the final day, we'll plan further development of this research agenda.</li>
            </ul>
        </section>

        <section id="sessions">
            <h2>Sessions</h2>
            <p>Sessions will include:</p>
            <ul>
                <li>Anders Sandberg's talk on theoretical limits to control: "Do any of them actually tell us anything?"
                </li>
                <li>Forrest Landry's whiteboard chat on an overlooked dynamic: "Virtual machines in recursive feedback"
                </li>
                <li>Richard Everheart's logical framework for AI alignment, aiming to refine foundational understanding.
                </li>
                <li>Thibaud Veron's session on a framework and engineered toy models that illustrate control dynamics.
                </li>
                <li>Will Petillo's session on better communication and narrative framings for AI safety/control
                    concepts.</li>
            </ul>
            <p>(More details on specific talks and activities will be added as confirmed.)</p>
        </section>

        <section id="proceedings">
            <h2>Proceedings, post-workshop outputs</h2>
            <p>This section will be updated after the workshop with summaries, key insights, and any public materials
                generated.</p>
            <p>Potential outputs may include:</p>
            <ul>
                <li>Summary report of discussions, key agreements, and open questions</li>
                <li>Notes or photos from sessions</li>
                <li>Links to research papers, blog posts, or pre-prints influenced by the workshop</li>
                <li>A refined list of open research questions in the "limits to control" domain</li>
                <li>Presentations or slide decks (if speakers consent to public sharing)</li>
            </ul>
        </section>

        <section id="join-us">
            <h2>Join us</h2>
            <p>This workshop is for researchers actively working on or deeply interested in the theoretical and
                practical limits of AI control. Do you wish to contribute to these focused discussions? Email Orpheus at
                <a href="mailto:o@horizonomega.org">o@horizonomega.org</a> to express your interest.
            </p>
            <p>Costs & funding: Participants are generally expected to cover their own travel and accommodation. We can
                reimburse only some whose research is not yet funded. The workshop has a grant offer from Survival and
                Flourishing Fund.</p>
            <p>To prepare: Read work by participants you are curious to chat with. Then we share some understanding
                already going in. Most of our time will be in collaborative discussions, so consider where you could
                bring in specific problems or concepts.</p>
        </section>

        <section id="suggested-reading">
            <h2>Suggested reading</h2>
            <p>Writings by participating researchers:</p>

            <h3>Papers:</h3>
            <ul>
                <li><a href="https://journals.riverpublishers.com/index.php/JCSANDM/article/view/16219/13165"
                        target="_blank">On the Controllability of Artificial Intelligence: An Analysis of
                        Limitations</a>, by Roman Yampolskiy</li>
                <li><a href="https://books.google.ca/books?hl=en&lr=&id=V3XsEAAAQBAJ&oi=fnd&pg=PT2&dq=info:aqKg8aByH3oJ:scholar.google.com&ots=j4-shaSx-8&sig=Q_L-K7LyOhOIziyr1mnmiP0zy38&redir_esc=y#v=onepage&q&f=false"
                        target="_blank">AI: Unexplainable, Unpredictable, Uncontrollable</a>, by Roman Yampolskiy</li>
                <li><a href="https://dl.acm.org/doi/10.1145/3603371#d1e583" target="_blank">Impossibility Results in
                        AI</a>, by Roman Yampolskiy</li>
                <li>[Forthcoming paper on control limits] by Anders Sandberg, Aybars Kocoglu, Thibaud Veron</li>
                <li>[Forthcoming paper on a logical framework] by Richard Everheart</li>
            </ul>

            <h3>Essays:</h3>
            <ul>
                <li><a href="https://www.alignmentforum.org/posts/NFYLjoa25QJJezL9f/lenses-of-control"
                        target="_blank">Lenses of Control</a>, by Will Petillo</li>
                <li><a href="https://www.lesswrong.com/posts/xp6n2MG5vQkPpFEBH/the-control-problem-unsolved-or-unsolvable"
                        target="_blank">The Control Problem: Unsolved or Unsolvable?</a>, by Remmelt Ellen</li>
                <li><a href="http://mflb.com/ai_alignment_1/tech_align_error_correct_fail_gld.html"
                        target="_blank">Control as a Causative Feedback Process</a>, by Forrest Landry</li>
                <li><a href="http://mflb.com/ai_alignment_1/si_safety_qanda_gld.html" target="_blank">An Exploration of
                        AGI Uncontainability</a>, by Forrest Landry</li>
                <li><a href="http://mflb.com/ai_alignment_1/agi_error_correction_gld.html" target="_blank">On Error
                        Detection</a>, by Forrest Landry</li>
            </ul>
            <p>Sorted roughly by ease of reading. To clarify an argument, do reach out to authors. Authors value
                questions!</p>
        </section>

        <hr>

        <section id="organizing-team">
            <h2>Organizing team</h2>
            <p>This event is hosted by <a href="https://horizonomega.org/" target="_blank">H&Omega;</a>, and organized
                by:</p>
            <ul>
                <li>Orpheus Lummis (H&Omega;)</li>
                <li>Remmelt Ellen</li>
                <li>Thibaud Veron</li>
            </ul>
        </section>

        <section id="contact">
            <h2>Contact</h2>
            <p>For inquiries regarding the workshop, please contact Orpheus at <a
                    href="mailto:o@horizonomega.org">o@horizonomega.org</a>.</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Limits to Control Workshop. All rights reserved.</p>
    </footer>

</body>

</html>