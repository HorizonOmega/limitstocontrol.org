<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Limits to Control Workshop 2025</title>
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Roboto+Slab:wght@400;700&display=swap" rel="stylesheet">
    <link href="main.css" rel="stylesheet">
</head>

<body>
    <header class="hero">
        <h1>Limits to Control Workshop 2025</h1>
    </header>

    <div class="statement-card">
        <a href="statement.html">
            <h2>Workshop Statement</h2>
            <p>Read the collective statement from the 2025 Limits to Control Workshop</p>
        </a>
    </div>

    <main class="container">
        <section id="intro">
            <h2>Limits to Control</h2>
            <p>Can we keep enough control over AI? If systems are developed to be more and more autonomous, this is no
                longer a given. It's a hypothesis, calling for serious investigation.</p>
            <p>Even alignment relies on control. Researchers build mechanisms to control AI's impacts in line with human
                values.</p>
            <p>So how must a control mechanism operate? What limits its capacity to track and correct all the AI
                signals/effects? Does it provide enough stability, or give way eventually to runaway impacts?</p>
            <p>We explore these questions in a new field: Limits to Control.</p>
        </section>

        <section id="about-workshop">
            <h2>About the workshop</h2>
            <p>This in-person workshop brought together researchers to map the territory of AI control limitations â€“ to understand the dynamics, patterns, and impossibilities of control. Through presentations, whiteboard discussions, and collaborative problem-solving, participants worked to build common knowledge, clarify viable research directions, and formalize this critical research topic.</p>
        </section>

        <section id="dates-location">
            <h2>Dates & location</h2>
            <ul>
                <li>Date: Wednesday, June 11 - Friday, June 13, 2025</li>
                <li>Location: University of Louisville, Kentucky, USA (Belknap Academic Building, rooms 427 & 428)</li>
            </ul>
        </section>

        <section id="proceedings">
            <h2>Proceedings</h2>
            <p>See <a href="statement.html">workshop statement</a>.</p>
            
            <p>Paper publications are pending. See for now:</p>
            <ul>
                <li>Richard Everheart's pre-print "On the Boundaries of Formal Intelligence"</li>
                <li>Remmelt Ellen's essay "Deconfusing AI and Evolution"</li>
            </ul>
        </section>

        <section id="participants">
            <h2>Participants</h2>
            <p>The workshop brought together researchers actively working on the theoretical and practical limits of AI control:</p>
            <ul>
                <li>Prof. Roman Yampolskiy</li>
                <li>Dr. Anders Sandberg</li>
                <li>Thibaud Veron</li>
                <li>Aybars Kocoglu</li>
                <li>Forrest Landry</li>
                <li>Richard Everheart</li>
                <li>Remmelt Ellen</li>
                <li>Will Petillo</li>
            </ul>
        </section>

        <section id="suggested-reading">
            <h2>Related reading</h2>
            <p>Writings by workshop participants:</p>

            <h3>Papers:</h3>
            <ul>
                <li><a href="https://journals.riverpublishers.com/index.php/JCSANDM/article/view/16219/13165"
                        target="_blank">On the Controllability of Artificial Intelligence: An Analysis of
                        Limitations</a>, by Roman Yampolskiy</li>
                <li><a href="https://books.google.ca/books?hl=en&lr=&id=V3XsEAAAQBAJ&oi=fnd&pg=PT2&dq=info:aqKg8aByH3oJ:scholar.google.com&ots=j4-shaSx-8&sig=Q_L-K7LyOhOIziyr1mnmiP0zy38&redir_esc=y#v=onepage&q&f=false"
                        target="_blank">AI: Unexplainable, Unpredictable, Uncontrollable</a>, by Roman Yampolskiy</li>
                <li><a href="https://dl.acm.org/doi/10.1145/3603371#d1e583" target="_blank">Impossibility Results in
                        AI</a>, by Roman Yampolskiy</li>
                <li>Richard Everheart's pre-print: "On the Boundaries of Formal Intelligence"</li>
                <li>Remmelt Ellen's essay: "Deconfusing AI and Evolution"</li>
                <li>[Forthcoming paper on control limits] by Anders Sandberg, Aybars Kocoglu, Thibaud Veron</li>
            </ul>

            <h3>Essays:</h3>
            <ul>
                <li><a href="https://www.alignmentforum.org/posts/NFYLjoa25QJJezL9f/lenses-of-control"
                        target="_blank">Lenses of Control</a>, by Will Petillo</li>
                <li><a href="https://www.lesswrong.com/posts/xp6n2MG5vQkPpFEBH/the-control-problem-unsolved-or-unsolvable"
                        target="_blank">The Control Problem: Unsolved or Unsolvable?</a>, by Remmelt Ellen</li>
                <li><a href="http://mflb.com/ai_alignment_1/tech_align_error_correct_fail_gld.html"
                        target="_blank">Control as a Causative Feedback Process</a>, by Forrest Landry</li>
                <li><a href="http://mflb.com/ai_alignment_1/si_safety_qanda_gld.html" target="_blank">An Exploration of
                        AGI Uncontainability</a>, by Forrest Landry</li>
                <li><a href="http://mflb.com/ai_alignment_1/agi_error_correction_gld.html" target="_blank">On Error
                        Detection</a>, by Forrest Landry</li>
            </ul>
        </section>

        <hr>

        <section id="organizing-team">
            <h2>Organizing team</h2>
            <p>This event was hosted by <a href="https://horizonomega.org/" target="_blank">H&Omega;</a>, and organized
                by:</p>
            <ul>
                <li>Orpheus Lummis (H&Omega;)</li>
                <li>Remmelt Ellen</li>
                <li>Thibaud Veron</li>
            </ul>
        </section>

        <section id="contact">
            <h2>Contact</h2>
            <p>For inquiries, please contact Orpheus at <a
                    href="mailto:o@horizonomega.org">o@horizonomega.org</a>.</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 Limits to Control Workshop. All rights reserved.</p>
    </footer>

</body>

</html>